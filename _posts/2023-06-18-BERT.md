---
title:  "[NLP] BERT"
excerpt: "Brief Introduction of BERT"
layout: single
author_profile: true

categories:
  - NLP
tags:
  - [NLP, BERT]
 
date: 2023-06-18
last_modified_at: 2023-06-18

use_math: true

sidebar:
  nav: "docs"
---

Bidirectional Encoder Representations from Transformers, also known as <b>BERT</b>, is a pre-trained model that was introduced in 2018 by Google. 

<h2>Background</h2>

BERT model is comprised of [transformers](https://jaehwan-c.github.io/nlp/Transformer/), with the dataset derived from Wikipedia and BookCorpus. For the performance, the model referred other cases, which consisted of hyperparameter tuning of pre-trained model with no label with the tasks with existing labels.

There are two models with different sizes:

1. BERT-Base, 12 stacks of encoders from transformers, $d_{model} = 768$, 12 Self-Attention Heads,
2. BERT-Large, 24 stacks of encoders from transformers, $d_{model} = 1024$, 16 Self-Attention Heads.

The former model consists of the number of hyperparameters same as GPT-1 model to make a comparison. The latter model is for the best performance of the model.

<h2>Components</h2>

This sector demonstrates the components of BERT model, including the word embedding, tokenizer, and positional embedding.

<h3>Embeddings</h3>

Considering the existence of $d_{model}$, BERT utilizes embedding layers with specific size. The output embedding refers itself from all the input given. The transformer of each layer comprises of Multi-head self-attention and Position-wise Feed Forward Neural Network.

<h3>Tokenizers</h3>

For tokenizing the words, there are two cases for the execution:

- When the token <b>exists</b> in the word list, do not divide the word into subwords
- When the token <b>does not exist</b> in the word list, the word is divided into subwords. If the divided subword is not the start of the word, "##" is added in front of the subword.

For instance, when the word "embeddings" is given, this word is not in the list of words. Therefore, it is divided into subwords "em," "##bed, "##ding," "##s." The word list is saved in a text file, named vocabulary.txt.

<h3>Positional Embedding</h3>

For assigning the positions for the words, BERT model applies positional embedding, which can be trained. The length of embedding vectors is 512, the maximum length of sentence for the input.

For each embedding vector, positional embedding vector is added.

<h3>Segment Embedding</h3>

This embedding is for the input with two sentences. For dividing the sentences, segment embedding layer is added.

For the first sentence, value of 0 is added. For the second sentence, value of 1 is added.

![BERT Model Embedding](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC7.PNG "BERT Model Embedding"){: .align-center}

This diagram is the overall demonstration of the embeddings that are explained and how they are given as input.

<h3>Attention Mask</h3>

![Attention Mask](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC11.PNG "Attention Mask"){: .align-center}

Attention Mask is a sequence input for separating the real word and the padding token for not putting mask for the padding token. Here, a value of 1 is added when it is a word.

<h2>Pre-training</h2>

![Pre-training](https://wikidocs.net/images/page/35594/bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png "Pre-training"){: .align-center}

This is the overall pre-training mechanism for other models also. The difference between GPT and BERT is the bi-directionality in the training process. This is obtained by masked language model.

<h3>Masked Language Model</h3>

For the training, around 15% of the input text is masked. After the masking, those words are processed with following logic:

- 80% of them - changed to [MASK]
- 10% of them - randomly changed
- 10% of them - remains the same

This is done for the problem of not matching of pre-training and fine-tuning processes for [MASK] token not appearing in fine-tuning process. 

<h3>Next Sentence Prediction</h3>

Another feature that BERT consists of is checking the continuality of the sentences A and B.

![NSP](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC10.PNG "NSP"){: .align-center}

For the end of each sentence, a [SEP] token is provided. At the output layer of [CLS] token, the model is to solve the problem for checking whether the second sentence is the sentence following the previous one.

<h2>Tasks</h2>

There are different tasks that BERT model can perform. 

<h3>Single Text Classification</h3>

![Single Text Classification](https://wikidocs.net/images/page/115055/apply1.PNG "Single Text Classification"){: .align-center}

This is for analyzing the text. The examples may be semantic analysis and news topic classification. For this task, [CLS] token is added at the beginning of the text for performing the classification.

<h3>Tagging</h3>

![Tagging](https://wikidocs.net/images/page/115055/apply2.PNG "Tagging"){: .align-center}

Another task is tagging. This is for analyzing each word. At each input word, there is a dense layer for the output for predicting the tags.

<h3>Text Pair Classification / Regression</h3>

![Text Pair](https://wikidocs.net/images/page/115055/apply3.PNG "Text Pair"){: .align-center}

This is for analyzing two sentences given as input. As mentioned previously, the dense layer at the beginning demonstrates the relationship between the former and the latter sentences.

<h3>Question Answering</h3>

![QA](https://wikidocs.net/images/page/115055/apply4.PNG "QA"){: .align-center}

Another task with two sentences as an input is the question answering task. For the second sentence, the context that is related to the answers of the question is provided. Then, the answer for the question is predicted based on the second sentence.

<h2>Conclusion</h2>

BERT model is another instance of large language model that is generated for the comparison with GPT. With different structure, this model is able to perrform for different tasks with acceptable level.

The overall paper can be found [here](https://arxiv.org/pdf/1810.04805.pdf).