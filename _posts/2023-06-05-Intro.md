---
title:  "[NLP] Intro"
excerpt: "Brief History of Natural Language Processing and What Will be Presented Here"
layout: single
author_profile: true

categories:
  - NLP
tags:
  - [NLP]
 
date: 2023-06-05
last_modified_at: 2023-06-05

sidebar:
  nav: "docs"
---

Here, under NLP categories, the historical flow of the overall model / process will be presented.

<h3>The timeline is as follows:</h3>

1. [Recurrent Neural Network (RNN)](https://jaehwan-c.github.io/nlp/RNN) - Rumelhart, 1986,
2. [Long Short-Term Memory (LSTM)](https://jaehwan-c.github.io/nlp/LSTM) - Hochreiter, Schmidhuber - 1997,
3. <u>Sequence to Sequence (Seq2Seq)</u> - Sutskever, Vinyals, Le - 2014,
4. <u>Attention is all you need (Transformer)</u> - Google - 2017
5. <u>GPT 1, 2, 3</u> - OpenAI - 2018 ~ 2020,
6. <u>Bidirectional Encoder Representations from Transformers</u> - Google - 2018.

Plus, some interesting models are to be reviewed and explained in simpler ways.